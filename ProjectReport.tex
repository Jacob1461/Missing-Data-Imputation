% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{report}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage[colorinlistoftodos]{todonotes}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Time Series Project},
  pdfauthor={Jacob Clinton George Smith-Kolff},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Time Series Project}
\author{Jacob Clinton George Smith-Kolff}
\date{2024-10-06}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{Introduction}\label{introduction}

(Probably work on this section last)

\section{What is missing data}\label{what-is-missing-data}

\section{Issues caused by missing
data}\label{issues-caused-by-missing-data}

\section{Structure of the report}\label{structure-of-the-report}

\chapter{Theory}\label{theory}

\section{Time series}\label{time-series}

\textbf{Missing at random. This basically means that the missingness in the data is not imformative}

We begin by providing definitions for both uni variate and multivariate
time series:

A univariate time series \(X = \{x_1, x_2, ..., x_t\} \in \mathbb{R}^t\)
is a sequence \(t\) observations on a single variable.\\
This can be extended to multivariate time series:\\
\(X = \{x_1, x_2, ..., x_t\}\in \mathbb{R}^{t\times d}\) where each
\(x_i\) is a \(d\) dimensional vector.

\section{Missing data}\label{missing-data}

\todo[inline]{remove the subsubsection numbers later}

Data missingness is a common occurrence that is common in working with
real world data. Missing data can arise from device failures such as
measureing equipment failing, or from data censoring (such as from
governments) {[}1{]}. Missing data typically falls into one of three
categories:

\subsection{Missing Completely at Random
(MCAR)}\label{missing-completely-at-random-mcar}

Data is said to be missing completely at random if the distribution that
describes how missingness occurs is independent of both the observed and
unobserved values in the time series.

\subsection{Missing at Random (MAR)}\label{missing-at-random-mar}

Missing at random is when missingness is related to the observed data,
but is independent of the unobserved data. This means that there is some
external factor that is causing the missingness. An example given in
{[}2{]} is that data from a sensor is more likely to be missing on
weekends since on some weekends the sensor is shutdown.

\subsection{Not Missing at Random
(NMAR)}\label{not-missing-at-random-nmar}

Not missing at random means that the missingness is related to the value
of the observation itself. An example of this is a sensor that will
return a missing value if the recorded value is above 100\(^\circ\).

\todo{Come back and include probability notation}

\section{Missing data imputation}\label{missing-data-imputation}

Most missing data imputation methods require MCAR or MAR, this is
because these systems of missingness are not informative towards the
missing values themselves. \todo[inline]{Can extend this writeup later.}

\todo{Idea is to start with the most basic techniques, then more the more advanced ones}

\subsection{Multivariate data imputation
techniques}\label{multivariate-data-imputation-techniques}

\subsubsection{Simple (naive?) methods}\label{simple-naive-methods}

\begin{itemize}
\item Mean/Median/Mode
\item Forward/backward fill
\item Linear interpolation
\item MA
\end{itemize}

\subsubsection{K-nearest neighbours}\label{k-nearest-neighbours}

K-Nearest neighbors is a simple non-parametric machine learning
algorithm, it can be used for either classification or regression. It
can be applied in a time series data imputation context to estimate
missing values in a time series.

\[\frac{1}{K}\sum_{j=1}^K Y_j\] Consider a missing value \(x_i\) in a
time series, k-nearest neighbors will impute a value for the missing
value by calculating the mean of the datapoints in the neighborhood
determined by a distance metric such as Euclidean distance. The
algorithm is simple and is shown to be quite effective {[}3{]}.
K-nearest neighbors algorithm has a hyper-parameter K, the number of
nearest (non missing) data points to consider. A choice of K that is too
large may lead to smoothing over temporal patters (under fitting) this
has the effect of ignoring potentially important seasonal patterns. On
the other hand a value of K that is too small could lead to being overly
sensitive to the noise. The value for K needs to be carefully chosen,
typically by cross validation, but in a time series context it could be
chosen using sliding windows and forward chaining. The curse of
dimensionality is an issue for K-nearest neighbors. With higher
dimension it becomes harder to find data points that are closer, this
can lead to an increased sensitivity to noise and misleading
imputations. The computational cost also increases rapidly with more
dimensons in the data making the algorithm less efficient. Another issue
related to computational cost, K-nearest neighbors is a lazy learning
algorithm so the complete dataset ends up being stored for large
datasets this can make the algorithm slow or even infeasible

\todo{Come back to this section later}

\subsubsection{Multivariate Imputation by Chained Equations
(MICE)}\label{multivariate-imputation-by-chained-equations-mice}

\subsubsection{General Adversial Networks
(GAM)}\label{general-adversial-networks-gam}

\subsection{Univariate data
imputation}\label{univariate-data-imputation}

\subsubsection{The struggle with univariate data
imputation}\label{the-struggle-with-univariate-data-imputation}

Unlike with a multivariate time series, a univariate time series is only
one sequence of observations \(\{y_1, y_2, ..., y_n\}\). This simplier
form actually leads to increased difficulty when it comes to imputing a
missing value of \(y\), since we can no longer exploit any dependencies
between the predictor variables in the series. With univariate
imputation we can only rely on the previous (or future) values for \(y\)
along with the implicit time variable \footnote{moritz2015comparison}.

\subsubsection{Last Observed Carried Forward (LOCF) Next Observation
Carried Backward
(NOCB)}\label{last-observed-carried-forward-locf-next-observation-carried-backward-nocb}

\footnote{ahn2022comparison}

\subsubsection{Mean/Median imputation}\label{meanmedian-imputation}

\subsubsection{ARIMA-based Imputation}\label{arima-based-imputation}

Missing data can be imputed using an ARIMA model
\[\left(1+ \sum_{i=1}^q \beta_i B^i\right)y(t)W(t)\] Where \(p\) is the
AR part, \(d\) is the degree of differencing, \(q\) is order of the
moving average part. In ARIMA based imputation the model will use the
rest of the series to impute the missing value, if there are several
missing values then each missing value is calculated one at a time and
the model will use the previously imputed values as well as the data in
the series to predict the next missing values \#\#\#\# Kalman filtering

\chapter{Illustration of the method
application}\label{illustration-of-the-method-application}

Using the methods of missing data in practice.

\section{The data}\label{the-data}

\section{The methods}\label{the-methods}

\chapter{Conclusion}\label{conclusion}

\phantomsection\label{refs}
\begin{CSLReferences}{0}{0}
\bibitem[\citeproctext]{ref-little2019statistical}
\CSLLeftMargin{{[}1{]} }%
\CSLRightInline{R. J. Little and D. B. Rubin, \emph{Statistical analysis
with missing data}, vol. 793. John Wiley \& Sons, 2019.}

\bibitem[\citeproctext]{ref-moritz2015comparison}
\CSLLeftMargin{{[}2{]} }%
\CSLRightInline{S. Moritz, A. Sardá, T. Bartz-Beielstein, M. Zaefferer,
and J. Stork, {``Comparison of different methods for univariate time
series imputation in r,''} \emph{arXiv preprint arXiv:1510.03924},
2015.}

\bibitem[\citeproctext]{ref-ahn2022comparison}
\CSLLeftMargin{{[}3{]} }%
\CSLRightInline{H. Ahn, K. Sun, K. P. Kim, \emph{et al.}, {``Comparison
of missing data imputation methods in time series forecasting,''}
\emph{Computers, Materials \& Continua}, vol. 70, no. 1, pp. 767--779,
2022.}

\end{CSLReferences}

\chapter{Appendix}\label{appendix}

\end{document}
