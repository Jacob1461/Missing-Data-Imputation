```{r, include=FALSE, warning=FALSE}
library(bookdown)
library(dplyr)
library(knitr)
library(kableExtra)
```


# Illustration of the method application

In this section we define the datasets that are used in our analysis of missing data imputation. We begin with the datasets used in the univariate tests, then the multivariate dataset. We then discuss the results and analyse the performance of the different data imputation techniques.

## The data \label{section:the_data}

**Sunspots**  

The sunspots dataset reports the monthly mean relative sunspot numbers from 1749 to 1983 [@sunspots_Dataset]. There are 2820 observations. This series has no clear trend, but does exhibit seasonality with a roughtly 11 year cycle.

**Lake Huron**

This dataset reports the annual level of Lake Huron (located in Canada and the United States) measured in feet [@LakeHuron_Dataset]. There are 98 observations in the dataset. This dataset exhibits a downward trend and no clear seasonality.

**Beer Sales**

This dataset reports the monthly beer sales from 1975 to 1990 in millions of barrels [@Beersales_Dataset]. There are 192 observations. This dataset shows a clear trend and seasonality.

**Lynx**

This dataset records the number of Lynx that are trapped at the Mackenize River district in Canada from 1821 to 1934 [@Lynx_Dataset1; @Lynx_Dataset2; @Lynx_Dataset3]. This dataset has an upward trend and with a cycle of about 7 years.

The plots for univariate time series can be found in Figure \ref{fig:timeseries}.

**Stock Market Prediction**
The data used for multivariate imputation was a subset of CNN data on [kaggle](https://www.kaggle.com/datasets/ehoseinz/cnnpred-stock-market-prediction). This subset was NASDAQ data comprising of features from various categories of technical indication. There are 81 variables, from those, there are 80 variables with missingness, within those 80 variables, there is a cumulative percentage of 1.88% missing data. $n=1984$ is the number of values that need imputing. There are $105531$ data-points in total. Let it be known that we do not have the ground truths for the original data.

## The methods

### Univariate

For each of the univariate datasets introduced in Section \ref{section:the_data}, the missMethods package was used to generate MCAR missing values. For each dataset we generated seven different series with varying percentages of missing values from 10\% up to 70\%.
Since the original datasets are know, we used root mean squared error (RMSE) as an error metric. RMSE is defined as \begin{equation}
RMSE = \sqrt{\sum_{i=1}^n \frac{(y-\hat{y})^2}{n}}. \end{equation} 
If a missing value could not be imputed (for example LOCF cannot interpolate a missing value if it the first value) then the RMSE is not computed for that data point. Since there is randomness involved in generating the missing value, we repeated each imputation 10 times with different seeds, then averaged the results.


### MICE

We run MICE imputation for 25 iterations, or to convergence, whatever happened first, as there was a large amount of data in the NASDAQ dataset. Convergence is essential when imputing with MICE so the high level of iterations meant we could reach convergence. In practice, 10 is typically used. Although that would commonly be seen on a much smaller dataset.

### GAN

We train the GAN on the features of the NASDAQ data. Training was conducted for 1500 epochs with mini batch sizes of 128.

In each epoch:

-   A batch of real data is selected, which passes through the generator to generate synthetic data.

-   The discriminator is trained using real and synthetic values, it receives real labels for the real data and receives fake labels for the synthetic data.

-   After training the discriminator, the generator is optimised to minimise the adversarial loss (fool our discriminator). And reconstruction loss (ensure the generated data is resembling our generated data).

Our hyper parameters for the model was *hidden_size=128, latent_size=256, epochs=1500, batch_size=128*.

### KNN 

K-nearest neighbours was the last multivariate method used for imputing data. $K=5$ was the value we used. This achieves a good balance between accurancy and time, as k grows there are more centoids to compute. Selecting a higher K could impose greater computational loads but have $K=1$ would be overfitting the data. 

### Quality of results  

#### Univariate  

Figure \ref{fig:Univarite_comparison} shows that every imputation technique suffers from increased RMSE as the percentage of missingness is increased this is to be expected, there does appear to be a mostly linear increase in RMSE as data missingness is increased. The primary interest is which imputation techniques dominate in terms of minimizing RMSE. Figure \ref{fig:Univarite_comparison} shows that mean imputation and LOCF are the the worst performing techniques in every test. Mean imputation always starts with the highest RMSE, but as the proportion of missing values are increased mean imputation beats out LOCF in the Beer sales and Lynx datasets. Mean imputation was by far the worst performing method on the sunspots dataset. The Kalman filter and linear interpolation methods both had very good performance across all datasets. These methods were also less affected by increasing the missingness as opposed to mean imputation and LOCF. The kalman filter or linear interpolation imputation technique would be recommended over LOCF and mean imputation, as even when the series is optimal for mean or LOCF, Kalman filtering or linear interpolation still outperformed them.
I also provide the RMSE values for each dataset in Tables \ref{tab:RMSE_sunspots}, \ref{tab:RMSE_lake}, \ref{tab:RMSE_beer}, \ref{tab:RMSE_lynx}.

```{r child='chapter3_content/univariate_figs.Rmd'}
```


#### Multivariate
After imputation, we use Kernel Density Estimation (KDE) to estimate the true distribution for the variables in our data, as well as our imputation methods. KDE provides a smooth estimation for the true distribution.

The Kullback-Leibler (KL) divergence is used to measure the difference between the distributions of our imputed data (GAN, MICE, and KNN) against our original data (with missingness). Since the ground truth for our original data is not known, this comparison helps us asses how well our imputation methods match the inferred distribution of our original data.

The KL Divergence is calculated for each variable, and each method that went through imputation. We then take the mean over all the variables for each method to get an overall average and assessment of how well our imputation methods work.

```{r, echo=FALSE, warning=FALSE}

kl_metrics <- read.csv("../metrics/kl_divergence_metrics.csv")
kl_metrics_rounded <- kl_metrics %>% 
  mutate_if(is.numeric, ~ round(., 4))

kl_metrics_rounded %>% 
  head() %>%
  kable(caption = "KL Divergence Metrics \\label{tab:kl_divergence}")

```

By doing this, we can try and gain an understanding into how well our methods have worked while having the absence of ground truths in our data.

```{r times-table, echo=FALSE, warning=FALSE}

times <- read.csv("../metrics/times_multivariate.csv")


kable(times, caption = "Imputation Method Times \\label{tab:training_times}")

```

From Table \ref{tab:training_times}, we can see that KNN was the quickest method out of the three chosen methods. KNN was approximately 150 times faster than MICE. Moreover, it was approximately 250 times faster than the GAN. MICE still had the lowed KL-Divergence score out of the three methods. 
```{r, echo=FALSE, warning=FALSE}

kl_means <- read.csv("../metrics/mean_kl_divergence.csv")

kl_means_rounded <- kl_means %>% 
  mutate_if(is.numeric, ~ round(., 4))

kable(kl_means_rounded, caption = "Mean KL Divergence Means \\label{tab:kl_div_means}")

```

Overall, MICE provided us with the best results in the quickest period of times. 
Table \ref{tab:kl_div_means} summarises the mean KL divergence for each method. Further training and hyper parameter tuning of the GAN could yield better results but that would need to be conducted in further research. 



