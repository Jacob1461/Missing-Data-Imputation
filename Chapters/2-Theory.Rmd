---
editor_options: 
  markdown: 
    wrap: 72
---

# Theory

## Time series definition

\textbf{Missing at random. This basically means that the missingness in the data is not imformative}

We begin by providing definitions for both uni variate and multivariate
time series:

A univariate time series $X = \{x_1, x_2, ..., x_t\} \in \mathbb{R}^t$
is a sequence $t$ observations on a single variable.\
This can be extended to multivariate time series:\
$X = \{x_1, x_2, ..., x_t\}\in \mathbb{R}^{t\times d}$ where each $x_i$
is a $d$ dimensional vector.

## Missing data Mechanisms

Data missingness is a common occurrence that is common in working with
real world data. Missing data can arise from device failures such as
measureing equipment failing, or from data censoring (such as from
governments) [@little2019statistical]. Missing data typically falls into
one of three categories:

### Missing Completely at Random (MCAR)

Data is said to be missing completely at random if the distribution that
describes how missingness occurs is independent of both the observed and
unobserved values in the time series.

### Missing at Random (MAR)

Missing at random is when missingness is related to the observed data,
but is independent of the unobserved data. This means that there is some
external factor that is causing the missingness. An example given in
[@moritz2015comparison] is that data from a sensor is more likely to be
missing on weekends since on some weekends the sensor is shutdown.

### Not Missing at Random (NMAR)

Not missing at random means that the missingness is related to the value
of the observation itself. An example of this is a sensor that will
return a missing value if the recorded value is above 100$^\circ$.

\todo{Come back and include probability notation}

## Missing data imputation

Most missing data imputation methods require MCAR or MAR, this is
because these systems of missingness are not informative towards the
missing values themselves. \todo[inline]{Can extend this writeup later.}

\todo{Idea is to start with the most basic techniques, then more the more advanced ones}

### Univariate data imputation

#### The struggle with univariate data imputation

Unlike with a multivariate time series, a univariate time series is only
one sequence of observations $\{y_1, y_2, ..., y_n\}$. This simplier
form actually leads to increased difficulty when it comes to imputing a
missing value of $y$, since we can no longer exploit any dependencies
between the predictor variables in the series. With univariate
imputation we can only rely on the previous (or future) values for $y$
along with the implicit time variable [^1].

[^1]: moritz2015comparison

#### Last Observed Carried Forward (LOCF) Next Observation Carried Backward (NOCB)

[^2]

[^2]: ahn2022comparison

#### Mean/Median imputation
This is another simple approach for filling in a missing value. We take the mean over the entire series and impute the missing value with the series mean:
\begin{equation}\label{eqn:Mean_imputation}
\hat{\mu} = \sum_i \frac{y_i}{n}
\end{equation}

If we replace Equation \ref{eqn:Mean_imputation} with the median, then we have median imputation.  

\todo[inline]{Keep talking about why this method is not great in general and why it could be worse for series with a trend}

#### Spline interpolation

A split interpolation of missing data assumes a linear relationship
between the data points. The equation is given by:
\begin{equation}\label{eqn:Split_interp}
f(x) = f(x_0) + \frac{f(x_1)-f(x_0)}{x_1-x_0}(x_1 - x_0)
\end{equation}

#### Kalman filter 
### Multivariate data imputation techniques

#### K-nearest neighbours

K-Nearest neighbours is a simple non-parametric machine learning
algorithm, it can be used for either classification or regression. It
can be applied in a time series data imputation context to estimate
missing values in a time series.

$$\frac{1}{K}\sum_{j=1}^K Y_j$$ Consider a missing value $x_i$ in a time
series, k-nearest neighbours will impute a value for the missing value
by calculating the mean of the data points in the neighbourhood
determined by a distance metric such as Euclidean distance. The
algorithm is simple and is shown to be quite effective
@ahn2022comparison. K-nearest neighbours algorithm has a hyper-parameter
K, the number of nearest (non missing) data points to consider. A choice
of K that is too large may lead to smoothing over temporal patters
(under fitting) this has the effect of ignoring potentially important
seasonal patterns. On the other hand a value of K that is too small
could lead to being overly sensitive to the noise. The value for K needs
to be carefully chosen, typically by cross validation, but in a time
series context it could be chosen using sliding windows and forward
chaining. The curse of dimensionality is an issue for K-nearest
neighbours. With higher dimension it becomes harder to find data points
that are closer, this can lead to an increased sensitivity to noise and
misleading imputations. The computational cost also increases rapidly
with more dimensions in the data making the algorithm less efficient.
Another issue related to computational cost, K-nearest neighbours is a
lazy learning algorithm so the complete dataset ends up being stored.
For large datasets this can make the algorithm slow, or even infeasible

\todo{Come back to this section later}

#### Multivariate Imputation by Chained Equations (MICE)

MICE is a common imputation technique that is quick to incorporate and
yields effective results. The algorithm begins by using something simple
like mean imputation, to fill in initial missing values.

For each variable where missingness is present, we build a regression
model. From there, lagged values ${y_{t-1}, y_{t-2}...,y_{t-n}}$ as
predictors. This is to say that if ${y(t)}$ is missing. predict using
$y_{t-1}$ and other variables.

impute the missing values for one variable at a time. After that
variable is imputed, use the updated data and move on to the next
variable. We repeat this process until we reach convergence for our
given variable.

This chained process is typically repeated 5-10 times to generate
multiple datasets that contain a variation of results.

After, each data set that has undergone imputation is analysed. Then, we
combine the results using Rubin's rule.

Rubin's rule is as follows:

1.  **Pooled Estimate** (e.g., mean, regression coefficient):

    $$\hat{\theta} = \frac{1}{m}\sum_{i=1}^{m}\theta_{i}$$

where $\hat{\theta}$ is the pooled estimate, $m$ is the number of
imputed datasets, and $\theta_i$ is the estimate in each imputed dataset
$i$.

#### General Adversarial Networks (GANs)

GANs consist of two neural networks. A generator $(G)$, and a
discriminator $(D)$ . These two networks are trained simultaneously
through a game theory approach. It is the generators job to try and
generate realistic data points, while the discriminator tries to
distinguish whether the values are true values or values generated from
the generator. The generator is tasked to try and fool the
discriminator. In turn, attempting to learn the underlying distribution
of the data. This class of machine learning models were first purposed
in 2014 [@NIPS2014_5ca3e9b1].

The typical process of using a GAN is as follows:

1)  The generator produces an estimate for the missing values
    conditioned on **all** available observed data points.

2)  The discriminator evaluates the quality of the imputed value
    compared to real observed values.

3)  Both networks are trained to improve the generators ability to fool
    the discriminator.

The training process continues until the generator can emulate realistic
data that will fool the discriminator and in turn, replicate the
distribution of the observed data. Given this, GANs can provide a
powerful architecture for imputing missing values. Particularly in cases
where imputing techniques can struggle to model the complex
relationships within the data [@NIPS2014_5ca3e9b1].

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{graphs/gan_architecture.png}


\caption{Structure of our GAN for TS imputation}
\label{fig:Structure_of_gan}  % Corrected spelling
\end{figure}

In our GAN, we use a standard Generator $(G)$and Discriminator $(D)$ to
handle missing value imputation. The generator receives a sampled vector
from the latent space $(Z)$ and uses that to estimate missing data
points. The discriminator then evaluates whether the generated data
follows the true observed data. Trained using a minimax game where the
generator tries to keep fooling the discriminator.
Not missing at random means that the missingness is related to the value
of the observation itself. An example of this is a sensor that will
return a missing value if the recorded value is above 100$^\circ$.

\todo{Come back and include probability notation}

## Missing data imputation

Most missing data imputation methods require MCAR or MAR, this is
because these systems of missingness are not informative towards the
missing values themselves. \todo[inline]{Can extend this writeup later.}

\todo{Idea is to start with the most basic techniques, then more the more advanced ones}

### Multivariate data imputation techniques

#### Simple (naive?) methods

```{=tex}
\begin{itemize}
\item Mean/Median
\item Forward/backward fill
\item Linear interpolation
\end{itemize}
```
#### K-nearest neighbours

K-Nearest neighbours is a simple non-parametric machine learning
algorithm, it can be used for either classification or regression. It
can be applied in a time series data imputation context to estimate
missing values in a time series.

$$\frac{1}{K}\sum_{j=1}^K Y_j$$ Consider a missing value $x_i$ in a time
series, k-nearest neighbours will impute a value for the missing value
by calculating the mean of the data points in the neighbourhood
determined by a distance metric such as Euclidean distance. The
algorithm is simple and is shown to be quite effective
@ahn2022comparison. K-nearest neighbours algorithm has a hyper-parameter
K, the number of nearest (non missing) data points to consider. A choice
of K that is too large may lead to smoothing over temporal patters
(under fitting) this has the effect of ignoring potentially important
seasonal patterns. On the other hand a value of K that is too small
could lead to being overly sensitive to the noise. The value for K needs
to be carefully chosen, typically by cross validation, but in a time
series context it could be chosen using sliding windows and forward
chaining. The curse of dimensionality is an issue for K-nearest
neighbours. With higher dimension it becomes harder to find data points
that are closer, this can lead to an increased sensitivity to noise and
misleading imputations. The computational cost also increases rapidly
with more dimensions in the data making the algorithm less efficient.
Another issue related to computational cost, K-nearest neighbours is a
lazy learning algorithm so the complete dataset ends up being stored.
For large datasets this can make the algorithm slow, or even infeasible

\todo{Come back to this section later}

#### Multivariate Imputation by Chained Equations (MICE)

MICE is a common imputation technique that is quick to incorporate and
yields effective results. The algorithm begins by using something simple
like mean imputation, to fill in initial missing values.

For each variable where missingness is present, we build a regression
model. From there, lagged values ${y_{t-1}, y_{t-2}...,y_{t-n}}$ as
predictors. This is to say that if ${y(t)}$ is missing. predict using
$y_{t-1}$ and other variables.

impute the missing values for one variable at a time. After that
variable is imputed, use the updated data and move on to the next
variable. We repeat this process until we reach convergence for our
given variable. Reaching convergence is important because it shows that the values we are imputing have stabilized. 

This chained process is typically repeated 10 times to generate
multiple datasets that contain a variation of results.



#### General Adversarial Networks (GANs)

GANs consist of two neural networks. A generator $(G)$, and a
discriminator $(D)$ . These two networks are trained simultaneously
through a game theory approach. It is the generators job to try and
generate realistic data points, while the discriminator tries to
distinguish whether the values are true values or values generated from
the generator. The generator is tasked to try and fool the
discriminator. In turn, attempting to learn the underlying distribution
of the data. This class of machine learning models were first purposed
in 2014 [@NIPS2014_5ca3e9b1].

The typical process of using a GAN is as follows:

1)  The generator produces an estimate for the missing values
    conditioned on **all** available observed data points.

2)  The discriminator evaluates the quality of the imputed value
    compared to real observed values.

3)  Both networks are trained to improve the generators ability to fool
    the discriminator.

The training process continues until the generator can emulate realistic
data that will fool the discriminator and in turn, replicate the
distribution of the observed data. Given this, GANs can provide a
powerful architecture for imputing missing values. Particularly in cases
where imputing techniques can struggle to model the complex
relationships within the data [@NIPS2014_5ca3e9b1].


\begin{figure}
\centering
\includegraphics{graphs/gan_architecture.png}

\caption{Structure of a generative adversial network}
\label{fig:GAN_str}
\end{figure}
In our GAN, we use a standard Generator $(G)$and Discriminator $(D)$ to
handle missing value imputation. The generator receives a sampled vector
from the latent space $(Z)$ and uses that to estimate missing data
points. The discriminator then evaluates whether the generated data
follows the true observed data. Trained using a minimax game where the
generator tries to keep fooling the discriminator.

### Univariate data imputation

#### The struggle with univariate data imputation

Unlike with a multivariate time series, a univariate time series is only
one sequence of observations $\{y_1, y_2, ..., y_n\}$. This simpler
form actually leads to increased difficulty when it comes to imputing a
missing value of $y$, since we can no longer exploit any dependencies
between the predictor variables in the series. With univariate
imputation we can only rely on the previous (or future) values for $y$
along with the implicit time variable [^1].

[^1]: moritz2015comparison

#### Last Observed Carried Forward (LOCF) Next Observation Carried Backward (NOCB)

[^2]

[^2]: ahn2022comparison

#### Mean/Median imputation

#### ARIMA-based Imputation

Missing data can be imputed using an ARIMA model
$$\left(1+ \sum_{i=1}^q \beta_i B^i\right)y(t)W(t)$$ Where $p$ is the AR
part, $d$ is the degree of differencing, $q$ is order of the moving
average part. In ARIMA based imputation the model will use the rest of
the series to impute the missing value, if there are several missing
values then each missing value is calculated one at a time and the model
will use the previously imputed values as well as the data in the series
to predict the next missing values \#### Kalman filtering
