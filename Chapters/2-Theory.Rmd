---
editor_options: 
  markdown: 
    wrap: 72
---

# Theory

## Time series definition

We begin by providing definitions for both uni variate and multivariate
time series:

A univariate time series $X = \{x_1, x_2, ..., x_t\} \in \mathbb{R}^t$
is a sequence $t$ observations on a single variable.\
This can be extended to multivariate time series:\
$X = \{x_1, x_2, ..., x_t\}\in \mathbb{R}^{t\times d}$ where each $x_i$
is a $d$ dimensional vector.

## Missing data Mechanisms

Data missingness is a common occurrence that is common in working with
real world data. Missing data can arise from device failures such as
measureing equipment failing, or from data censoring (such as from
governments) [@little2019statistical]. Missing data typically falls into
one of three categories:

### Missing Completely at Random (MCAR)

Data is said to be missing completely at random if the distribution that
describes how missingness occurs is independent of both the observed and
unobserved values in the time series.

### Missing at Random (MAR)

Missing at random is when missingness is related to the observed data,
but is independent of the unobserved data. This means that there is some
external factor that is causing the missingness. An example given in
[@moritz2015comparison] is that data from a sensor is more likely to be
missing on weekends since on some weekends the sensor is shutdown.

### Not Missing at Random (NMAR)

Not missing at random means that the missingness is related to the value
of the observation itself. An example of this is a sensor that will
return a missing value if the recorded value is above 100$^\circ$.


## Missing data imputation

Most missing data imputation methods require MCAR or MAR, this is
because these systems of missingness are not informative towards the
missing values themselves. 


### Univariate data imputation \label{Section:univariateimputation}


#### Last Observed Carried Forward
Last Observed Carried Forward (LOCF) is a simple imputation method were a missing data point $y_j$ is replaced with the previous value $y_{j-1}$. This approach can work fine in situations where there is little missing data and if the missing values are not clustered close together. However, this method will fail to work if the first data point is missing. If in impute a missing value $y_j$ with the next value $y_{j+1}$ this is called Next observed carried backward (NOCB).


#### Mean imputation
Another simple approach to filling in missing data in a series is by using mean imputation. We use the equation \begin{equation}
\overline{y} = \sum_{i=1}^k \frac{y_i}{k}
\end{equation}
this is going to replace all the missing values of the dataset for the mean of the series. This is going to remove autocorrelation as well as introduce bias towards the mean of the series. Mean imputation can work if there a few missing values and the series is stationary with no seasonality.

#### Spline interpolation

A split interpolation of missing data assumes a linear relationship
between the data points. The equation is given by:
\begin{equation}\label{eqn:Split_interp}
f(x) = f(x_0) + \frac{f(x_1)-f(x_0)}{x_1-x_0}(x_1 - x_0)
\end{equation}

#### Kalman filter

### Multivariate data imputation techniques

Multivariate data imputation has the added benefit of being able to exploit dependencies between covariates when imputing missing data. Multivariate data can also exhibit the same temporal structure among variables to make imputation easier. There is an increased framework to handle multivariate missing data as this tends to be more common in practise. 

#### K-nearest neighbours (KNN)

K-Nearest neighbours is a simple non-parametric machine learning
algorithm, it can be used for either classification or regression. It
can be applied in a time series data imputation context to estimate
missing values in a time series. The equation is given by

\begin{equation}
\frac{1}{K}\sum_{j=1}^K y_j.
\end{equation}
To impute a missing data point we consider the mean value of the K nearest neighbours by a distance measure. This approach works well for the correct choice of $k$, but the downside is that this is a hyper-parameter and has to be chosen, perhaps by cross-validation. Another downside is that KNN requires the entire dataset to be stored in memory, which can be infeasible for large datasets.

#### Multivariate Imputation by Chained Equations (MICE)

MICE is a common imputation technique that is quick to incorporate and
yields effective results. The algorithm begins by using something simple
like mean imputation, to fill in initial missing values.

For each variable where missingness is present, we build a regression
model. From there, lagged values ${y_{t-1}, y_{t-2}...,y_{t-n}}$ as
predictors. This is to say that if ${y(t)}$ is missing. predict using
$y_{t-1}$ and other variables.

impute the missing values for one variable at a time. After that
variable is imputed, use the updated data and move on to the next
variable. We repeat this process until we reach convergence for our
given variable. Reaching convergence is important because it shows that the values we are imputing have stabilized. 

This chained process is typically repeated 10 times to generate
multiple datasets that contain a variation of results.

MICE also assumes MCAR, if MCAR is not present, results may be biased. 



#### General Adversarial Networks (GANs)

GANs consist of two neural networks. A generator $(G)$, and a
discriminator $(D)$ . These two networks are trained simultaneously
through a game theory approach. It is the generators job to try and
generate realistic data points, while the discriminator tries to
distinguish whether the values are true values or values generated from
the generator. The generator is tasked to try and fool the
discriminator. In turn, attempting to learn the underlying distribution
of the data. This class of machine learning models were first purposed
in 2014 [@NIPS2014_5ca3e9b1].

The typical process of using a GAN is as follows:

1)  The generator produces an estimate for the missing values
    conditioned on **all** available observed data points.

2)  The discriminator evaluates the quality of the imputed value
    compared to real observed values.

3)  Both networks are trained to improve the generators ability to fool
    the discriminator.

The training process continues until the generator can emulate realistic
data that will fool the discriminator and in turn, replicate the
distribution of the observed data. Given this, GANs can provide a
powerful architecture for imputing missing values. Particularly in cases
where imputing techniques can struggle to model the complex
relationships within the data [@NIPS2014_5ca3e9b1].

\begin{figure}
\centering
\includegraphics{graphs/gan_architecture.png}

\caption{Structure of a generative adversial network}
\label{fig:GAN_str}
\end{figure}
In our GAN, we use a standard Generator $(G)$and Discriminator $(D)$ to
handle missing value imputation. The generator receives a sampled vector
from the latent space $(Z)$ and uses that to estimate missing data
points. The discriminator then evaluates whether the generated data
follows the true observed data. Trained using a minimax game where the
generator tries to keep fooling the discriminator.









