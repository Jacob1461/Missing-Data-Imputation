---
editor_options: 
  markdown: 
    wrap: 72
---

# Theory  

In this section we provide an introduction to both univariate and multivariate time series. We also introduce the imputation techniques that will be used.

## Time series definition

A univariate time series, represented as $X = \{x_1, x_2, ..., x_t\} \in \mathbb{R}^t$
is an ordered sequence of $t$ observations on a single variable. Each observation $x_i$ is indexed by a specific point in time. These observations can be either regularly space (for example daily, monthly, or yearly) or can be irregularly spaced.
We can extend the univariate case to a multivariate time series, represented as
$X = \{x_1, x_2, ..., x_t\}\in \mathbb{R}^{t\times d}$ where each $x_i$
is a $d$ dimensional vector. In this case each element in the time series consists of an ordered sequence of $t$ observations of several variables. We have for each observation $x_i = \{x_{i1}, x_{i2}, ..., x_{id}\}$ is a vector of $d$ values at each time point.

## Missing data Mechanisms

Missing data is a common occurrence and can be due to human error, machine error such as device failures, or data censoring where data is purposely removed [@little2019statistical]. Missing data falls into
one of three categories:

### Missing Completely at Random (MCAR)

Data is said to be missing completely at random if the distribution that
describes how missingness occurs is independent of both the observed and
unobserved values in the time series. This could be written as \begin{equation}
P(r|X_{observed},X_{missing}) = P(r)
\end{equation}
where $r$ is an indicator variable for missing data [@twumasi2019efficiency].
An example of MCAR could be some responses to a survey question where lost due to being corrupted during file transfer or storage The fact that those responses are missing has nothing to do with either the other observed files or the missing files themselves.

### Missing at Random (MAR)

Missing at random is when missingness is related to the observed data,
but is independent of the unobserved data. This means that there is some
external factor that is causing the missingness. An example given in
[@moritz2015comparison] is that data from a sensor is more likely to be
missing on weekends since on some weekends the sensor is shutdown. This mechanism of missingness could be written as \begin{equation}
P(r|X_{observed},X_{missing}) = P(r|X_{observed}).
\end{equation}

### Not Missing at Random (NMAR)

Not missing at random means that the missingness is related to the value
of the observation itself. An example of this is a sensor that will
return a missing value if the recorded value is above 100$^\circ c$.
This missingness mechanism can be expressed as \begin{equation}
P(r|X_{observed},X_{missing}) = P(r|X_{missing}).
\end{equation}
NMAR is the most complex case for missing data imputation and typically involves experimentation with the observed data to see how sensitive it is under various situations [@van2018flexible].  

## Missing data imputation

The data imputation methods considered for our project work best under the assumptions of MCAR or MAR. This is because, under these mechanisms, the missingness is not informative towards the missing values themselves. These methods may not be suitable in the case of NMAR. In the case of NMAR we may get biased results as NMAR provides some information relating to the missing values themselves, more complex techniques would have to be employed when dealing with this type of missingness mechanism.


### Univariate data imputation \label{Section:univariateimputation}


#### Last Observed Carried Forward
Last Observed Carried Forward (LOCF) is a simple imputation technique were a missing data point $x_j$ is replaced with the previous value $x_{j-1}$. This imputation technique does not require any assumptions on the series, but does work best for series where the underlying process is stable or changes slowly overtime. It also performs better when there are not several consecutive missing values. This would create a long period where the last non missing value is repeated. A similar imputation technique is called Next observed carried backward (NOCB). Rather than using $x_{j-1}$ to impute $x_j$, we use $x_{j+1}$. Because of the similarity we will only focus on LOCF for this report.



#### Mean imputation
Mean imputation is another simple approach for imputing missing values. The equation is given by \begin{equation}
\overline{x} = \sum_{i=1}^k \frac{x_i}{k}.
\end{equation}

With this method, all missing values of the dataset will be replaced with the mean of the series. Mean imputation typically requires the missing data mechanism to be MCAR since there are no patterns in MCAR missing data. Using mean imputation can reduce autocorrelation within a time series as it introduces constant values where missing values occur. Mean imputation is best performed on series where there is low variability in the observed data and the series is stationary. 

#### Linear interpolation

Linear interpolation uses the two non missing values on the left and right of a missing value in order to impute it. I use the equation as used in [@lepot2017interpolation]
\begin{equation}\label{eqn:Split_interp}
X_i = \frac{x_A - x_B}{a-b}(i-b) + x_B
\end{equation}
where $X_i$ is the interpolated data point given two non missing end points $x_A$ and $x_B$, with index $i$. A linear interpolation assumes there is a linear relationship between the points. Therefore, a linear interpolation is not effective across time series that exhibits a non-linear trend. This method is not suitable for large gaps in the series or if the first and or last values are missing (as that would be extrapolation).

#### Kalman filter  

The Kalman filter was introduced in 1960 by Rudolph Emil Kalman. The kalman filter is an algorithm that works within a state space model framework. It consists of a state equation which models the evolution of the system overtime, and a measurement equation which describes the relationship between the system state to the observed noisey measurements. A Kalman filter is an efficient and optimal estimator in terms of minimum mean squared error (MMSE), if the series has both Gaussian noise and the system exhibits linear behavior.
When used for missing data the Kalman filter uses the current state transition equation to predict the missing value. When observed data is encountered it will update the state estimate.


### Multivariate data imputation techniques

In a univariate time series there is only the observation and the time point, so in general univariate data imputation is more difficult. With a multivariate time series there are now several variables that relate to an observation at that time point. In many real world scenarios there are correlations between the predictors that can be exploited to fill in a missing value. As well as temporal relationships that can be present in multivariate time series data. 

#### K-nearest neighbours (KNN)

K-nearest neighbours is a simple non-parametric machine learning
algorithm, it can be used for either classification or regression tasks. For imputing missing data, KNN finds the K nearest neighbours of a missing data point based on a distance metric (typically euclidean distance). A value is imputed by averaging the values of those K neighbours. The equation is given as:

\begin{equation}
\frac{1}{K}\sum_{j=1}^K x_j
\end{equation}

where $x_j$ represents the $j^{th}$ nearest neighbour. Whilst KNN can show very strong imputation performance on multivariate time series, the largest downside is the hyper-parameter $K$. The value of $K$ must be appropriately chosen, as having a value of $K$ too small can lead to overfitting, whereas too large $K$ can lead to smoothing (underfitting). $K$ is typically chosen by cross-validation, which can be time consuming espcially for large datasets. Another downside to KNN is that it is a lazy learning algorithm, it will have to re-run everytime there is a missing value that needs to be imputed. KNN must also store the entire dataset in memory. This means that KNN is very computationally demaning, and can be very slow for large datasets.

#### Multivariate Imputation by Chained Equations (MICE)

MICE is a common imputation technique that is quick to incorporate and
yields effective results. The algorithm begins by using something simple
like mean imputation, to fill in initial missing values.

For each variable where missingness is present, we build a regression
model. From there, lagged values ${x_{t-1}, x_{t-2}...,x_{t-n}}$ as
predictors. This is to say that if ${x(t)}$ is missing. predict using
$x_{t-1}$ and other variables.

Impute the missing values for one variable at a time. After that
variable is imputed, use the updated data and move on to the next
variable. We repeat this process until we reach convergence for our
given variable. Reaching convergence is important because it shows that the values we are imputing have stabilized. 

This chained process is typically repeated 10 times to generate
multiple datasets that contain a variation of results.

MICE also assumes MCAR, if MCAR is not present, results may be biased. 


#### General Adversarial Networks (GANs)

GANs consist of two neural networks. A generator $(G)$, and a
discriminator $(D)$. These two networks are trained simultaneously
through a game theory approach. It is the generator's job to try and
generate realistic data points, while the discriminator tries to
distinguish whether the values are true values or values generated from
the generator. The generator is tasked to try and fool the
discriminator. In turn, attempting to learn the underlying distribution
of the data. This class of machine learning models were first purposed
in 2014 [@NIPS2014_5ca3e9b1].Should

The typical process of using a GAN is as follows:

1)  The generator produces an estimate for the missing values
    conditioned on **all** available observed data points.

2)  The discriminator evaluates the quality of the imputed value
    compared to real observed values.

3)  Both networks are trained to improve the generators ability to fool
    the discriminator.

The training process continues until the generator can emulate realistic
data that will fool the discriminator and in turn, replicate the
distribution of the observed data. Given this, GANs can provide a
powerful architecture for imputing missing values. Particularly in cases
where imputing techniques can struggle to model the complex
relationships within the data [@NIPS2014_5ca3e9b1]. A diagram of the GAN architecture is shown in Figure \ref{fig:GAN_str}.

In our GAN, we use a standard generator $(G)$ and discriminator $(D)$ to
handle missing value imputation. The generator receives a sampled vector
from the latent space $(Z)$ and uses that to estimate missing data
points. The discriminator then evaluates whether the generated data
follows the true observed data. Trained using a minimax game where the
generator tries to keep fooling the discriminator.









